# X√¢y D·ª±ng Apache Airflow Pipeline cho Thu Th·∫≠p v√† Tr·ª±c Quan H√≥a D·ªØ Li·ªáu E-Commerce: H·ªó Tr·ª£ Ra Quy·∫øt ƒê·ªãnh Kinh Doanh cho Nh√† B√°n H√†ng v√† ng∆∞·ªùi mua

## Cung C·∫•p Insight Th·ªã Tr∆∞·ªùng E-Commerce cho Nh√† B√°n H√†ng v√† ng∆∞·ªùi d√πng Tiki & Lazada

Pipeline t·ª± ƒë·ªông crawl, x·ª≠ l√Ω v√† tr·ª±c quan h√≥a d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ Tiki v√† Lazada s·ª≠ d·ª•ng Apache Airflow, PostgreSQL v√† Metabase.

## üìã M·ª•c L·ª•c

- [T·ªïng Quan](#-t·ªïng-quan)
- [Ki·∫øn Tr√∫c](#-ki·∫øn-tr√∫c)
- [C√†i ƒê·∫∑t](#-c√†i-ƒë·∫∑t)
- [S·ª≠ D·ª•ng](#-s·ª≠-d·ª•ng)
- [Database Schema](#-database-schema)
- [C·∫•u Tr√∫c Project](#-c·∫•u-tr√∫c-project)
- [Troubleshooting](#-troubleshooting)

## üéØ T·ªïng Quan

Pipeline n√†y th·ª±c hi·ªán c√°c c√¥ng vi·ªác sau:

1. **Extract**: Crawl d·ªØ li·ªáu s·∫£n ph·∫©m t·ª´ 2 ngu·ªìn
   - **Tiki**: S·ª≠ d·ª•ng API (nhanh, ·ªïn ƒë·ªãnh)
   - **Lazada**: S·ª≠ d·ª•ng Selenium (v√¨ c√≥ JavaScript rendering)

2. **Transform**: L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
   - Parse gi√° (lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát)
   - Chu·∫©n h√≥a s·ªë l∆∞·ª£ng b√°n (x·ª≠ l√Ω notation 'k')
   - Ph√¢n lo·∫°i brand v√† category
   - Extract review count v√† score

3. **Load**: L∆∞u v√†o PostgreSQL
   - Upsert v√†o b·∫£ng `all_products` (snapshot hi·ªán t·∫°i)
   - Track l·ªãch s·ª≠ trong `price_history` (time-series data)

4. **Notify**: G·ª≠i th√¥ng b√°o qua Slack
   - Success notification v·ªõi statistics
   - Error alerts v·ªõi log links

## üèóÔ∏è Ki·∫øn Tr√∫c

![Pipeline](image/pipeline.png)

### Components

- **Airflow Webserver**: Web UI (http://localhost:8080)
- **Airflow Scheduler**: L√™n l·ªãch v√† ƒëi·ªÅu ph·ªëi tasks
- **Airflow Worker**: Ch·∫°y ETL code (Celery executor)
- **PostgreSQL Metadata**: L∆∞u Airflow state
- **PostgreSQL Data Warehouse**: L∆∞u business data
- **Redis**: Message broker cho Celery
- **Metabase**: BI tool (http://localhost:3000)

## üöÄ C√†i ƒê·∫∑t

### Prerequisites

- Docker Desktop 4.0+
- Docker Compose 2.0+
- 8GB RAM minimum (16GB recommended)
- 20GB disk space

### B∆∞·ªõc 1: Clone Project

```bash
git clone <repo-url>
cd airflow-project1
```

### B∆∞·ªõc 2: C·∫•u H√¨nh Environment

Copy v√† ch·ªânh s·ª≠a `.env`:

```bash
cp .env.example .env
# S·ª≠a passwords trong .env (n·∫øu c·∫ßn)
```

### B∆∞·ªõc 3: Build v√† Start Services

```bash
# Build Docker image
docker-compose build

# Start t·∫•t c·∫£ services
docker-compose up -d

# Xem logs
docker-compose logs -f
```

### B∆∞·ªõc 4: Truy C·∫≠p Services

- **Airflow UI**: http://localhost:8080
  - Username: `airflow`
  - Password: `airflow`

- **Metabase**: http://localhost:3000
  - Setup l·∫ßn ƒë·∫ßu: Ch·ªçn PostgreSQL
  - Host: `postgres-data`
  - Port: `5432`
  - Database: `mydatabase`
  - Username: `myuser`
  - Password: `mypassword`

### B∆∞·ªõc 5: C·∫•u H√¨nh Airflow Connections

Trong Airflow UI, th√™m 2 connections:

#### 1. PostgreSQL Data Warehouse

```
Admin -> Connections -> Add Connection
- Connection Id: postgres_data_conn
- Connection Type: Postgres
- Host: postgres-data
- Schema: mydatabase
- Login: myuser
- Password: mypassword
- Port: 5432
```

#### 2. Slack Webhook (Optional)

```
Admin -> Connections -> Add Connection
- Connection Id: slack_webhook_conn
- Connection Type: HTTP
- Host: https://hooks.slack.com
- Password: <your-webhook-token>
```

## üíª S·ª≠ D·ª•ng

### Ch·∫°y Pipeline Manually

1. V√†o Airflow UI: http://localhost:8080
2. T√¨m DAG: `master_tiki_lazada_pipeline`
3. Click toggle ƒë·ªÉ enable DAG
4. Click "Trigger DAG" ƒë·ªÉ ch·∫°y ngay

### Schedule Automatic

Pipeline m·∫∑c ƒë·ªãnh ch·∫°y **@daily** (00:00 h√†ng ng√†y).

ƒê·ªÉ thay ƒë·ªïi schedule, s·ª≠a trong `dags/master_pipeline.py`:

```python
@dag(
    schedule_interval="@daily",  # Ho·∫∑c "0 2 * * *" cho 2:00 AM
    ...
)
```

### Xem Logs

```bash
# Logs c·ªßa t·∫•t c·∫£ services
docker-compose logs -f

# Logs c·ªßa service c·ª• th·ªÉ
docker-compose logs -f airflow-worker

# Task logs trong Airflow UI
# DAGs -> master_tiki_lazada_pipeline -> Task Instance -> View Log
```

## üìä Database Schema

### Table: `all_products`

Snapshot s·∫£n ph·∫©m hi·ªán t·∫°i (upsert daily).

| Column | Type | Description |
|--------|------|-------------|
| id | SERIAL | Primary key |
| name | TEXT | T√™n s·∫£n ph·∫©m |
| price | INT | Gi√° (VNƒê) |
| sold_count | INT | S·ªë l∆∞·ª£ng ƒë√£ b√°n |
| review_count | INT | S·ªë l∆∞·ª£ng ƒë√°nh gi√° |
| review_score | DECIMAL(3,2) | ƒêi·ªÉm ƒë√°nh gi√° (0-5) |
| brand | TEXT | Th∆∞∆°ng hi·ªáu (Apple, Samsung, ...) |
| category | TEXT | Lo·∫°i s·∫£n ph·∫©m (ƒêi·ªán tho·∫°i, Laptop, ...) |
| source | TEXT | Ngu·ªìn (Tiki, Lazada) |
| crawled_at | TIMESTAMP | Th·ªùi gian crawl |

**Unique constraint**: `(name, source)`

### Table: `price_history`

L·ªãch s·ª≠ gi√° v√† sold count theo ng√†y (time-series).

| Column | Type | Description |
|--------|------|-------------|
| id | SERIAL | Primary key |
| name | TEXT | T√™n s·∫£n ph·∫©m |
| source | TEXT | Ngu·ªìn |
| price | INT | Gi√° t·∫°i th·ªùi ƒëi·ªÉm |
| sold_count | INT | S·ªë l∆∞·ª£ng b√°n t·∫°i th·ªùi ƒëi·ªÉm |
| review_count | INT | S·ªë ƒë√°nh gi√° |
| review_score | DECIMAL(3,2) | ƒêi·ªÉm ƒë√°nh gi√° |
| brand | TEXT | Th∆∞∆°ng hi·ªáu |
| category | TEXT | Lo·∫°i s·∫£n ph·∫©m |
| crawl_date | DATE | Ng√†y crawl |
| created_at | TIMESTAMP | Timestamp t·∫°o record |

**Unique constraint**: `(name, source, crawl_date)`


## üìÅ C·∫•u Tr√∫c Project

```
airflow-project1/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ master_pipeline.py          # Main DAG definition
‚îú‚îÄ‚îÄ includes/
‚îÇ   ‚îú‚îÄ‚îÄ etl_tiki.py                 # Tiki ETL module (API)
‚îÇ   ‚îî‚îÄ‚îÄ etl_lazada.py               # Lazada ETL module (Selenium)
‚îú‚îÄ‚îÄ logs/                           # Airflow task logs (auto-generated)
‚îú‚îÄ‚îÄ docker-compose.yaml             # Docker services definition
‚îú‚îÄ‚îÄ Dockerfile                      # Custom Airflow image
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ .env                            # Environment variables (DO NOT COMMIT)
‚îú‚îÄ‚îÄ .env.example                    # Environment template (safe to commit)
‚îú‚îÄ‚îÄ .gitignore                      # Git ignore rules
‚îî‚îÄ‚îÄ README.md                       # Documentation
```

## üîç Troubleshooting

### Issue: "Permission Denied" khi start containers

**Solution**: Set AIRFLOW_UID trong `.env` b·∫±ng UID c·ªßa user hi·ªán t·∫°i

```bash
# Linux/Mac
echo "AIRFLOW_UID=$(id -u)" >> .env

# Windows WSL
echo "AIRFLOW_UID=1000" >> .env

# Restart
docker-compose down
docker-compose up -d
```

### Issue: ChromeDriver not found

**Solution**: Rebuild Docker image

```bash
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

### Issue: Task fails v·ªõi "Connection refused" (Postgres)

**Solution**: Ki·ªÉm tra connection trong Airflow UI

1. Admin -> Connections -> `postgres_data_conn`
2. Test connection
3. N·∫øu fail, check host = `postgres-data` (kh√¥ng ph·∫£i localhost)

### Issue: Out of Memory

**Solution**: TƒÉng Docker memory limit

- Docker Desktop -> Settings -> Resources
- Memory: TƒÉng l√™n 8GB+
- Restart Docker

### Issue: DAGs kh√¥ng xu·∫•t hi·ªán

**Solution**: Check Airflow logs

```bash
docker-compose logs airflow-scheduler | grep -i error
docker-compose logs airflow-webserver | grep -i error
```

Th∆∞·ªùng do syntax error trong DAG files.

### Reset Database

ƒê·ªÉ reset to√†n b·ªô data v√† start fresh:

```bash
# Stop v√† x√≥a containers + volumes
docker-compose down -v

# Start l·∫°i
docker-compose up -d
```

## ü§ù Contributing

1. Fork project
2. Create feature branch
3. Commit changes
4. Push to branch
5. Create Pull Request


## B·ªï sung
docker run --rm --entrypoint bash apache/airflow:2.8.1 -c "airflow db init >/dev/null 2>&1 && cat /opt/airflow/airflow.cfg" > config/airflow.cfg